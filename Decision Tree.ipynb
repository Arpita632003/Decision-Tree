{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e0db94-1df4-4c39-9b4a-ccd313b41427",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is a Decision Tree, and how does it work\n",
    "A decision tree is a supervised learning algorithm used for both classification and regression tasks. It has a hierarchical tree structure which consists of a root node, branches, internal nodes and leaf nodes. It It works like a flowchart help to make decisions step by step where:\n",
    "\n",
    "Internal nodes represent attribute tests\n",
    "Branches represent attribute values\n",
    "Leaf nodes represent final decisions or predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2225046-3138-4557-99a9-2371514ca970",
   "metadata": {},
   "outputs": [],
   "source": [
    " What are impurity measures in Decision Trees\n",
    "mpurity measures in Decision Trees are used to evaluate the quality of splits during tree construction. Common impurity measures include:\n",
    "Entropy: Measures the disorder or randomness in a set of data.\n",
    "Gini Index: Calculates the likelihood of an instance being incorrectly classified.\n",
    "Classification Error: Measures the proportion of misclassified instances in a subset of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec7fc86-a21f-44bd-bb6f-ef9f18210b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is the mathematical formula for Gini Impurity\n",
    "Then the Gini Impurity of $D$ is defined as: $$ Gini (D) = 1- sum_ {i=1}^k p_ {i}^ {2} $$ The node with uniform class distribution has the highest impurity. The minimum impurity is obtained when all records belong to the same class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9475f28-9b33-4828-966a-7d5114411858",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is the mathematical formula for Entropy\n",
    "The Entropy Formula is S = k * ln (W), where S is entropy, k is Boltzmann's constant, and W is the number of possible microstates of a system. This formula is used in various branches of physics, including thermodynamics, statistical mechanics, and information theory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35245b3-64cb-42a6-a106-6712d9ebe0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is Information Gain, and how is it used in Decision Trees\n",
    "Information Gain (IG) is a measure used in decision trees to quantify the effectiveness of a feature in splitting the dataset into classes. It calculates the reduction in entropy (uncertainty) of the target variable (class labels) when a particular feature is known.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8813b9-9b1e-469a-984e-b5689539cdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is the difference between Gini Impurity and Entropy\n",
    "n decision tree algorithms, Gini Index and Entropy are two popular methods used to measure the impurity or disorder of a dataset. Both methods help in selecting the best feature for splitting the data at each node of the tree, but they differ in their computation and properties.\n",
    "\n",
    "Gini Index\n",
    "\n",
    "The Gini Index, also known as Gini Impurity, calculates the probability of a randomly chosen instance being misclassified. It ranges from 0 to 1, where 0 indicates perfect purity and 1 indicates maximum impurity. The formula for Gini Impurity is:\n",
    "\n",
    "Gini = 1 - sum(p_i^2 for p_i in probabilities)\n",
    "where p_i is the proportion of instances of class i in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61d50b-ad71-4f8f-b29f-eccb31fdcf98",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is the mathematical explanation behind Decision Trees\n",
    " Decision trees are one of the most intuitive and powerful machine learning algorithms used for classification and regression problems. In this post, we will explore the mathematical foundations behind decision trees with a step-by-step example, covering concepts like entropy, information gain, and tree construction.\n",
    "\n",
    "Understanding Decision Trees\n",
    "A decision tree is a flowchart-like structure where:\n",
    "\n",
    "Each internal node represents a decision based on a feature.\n",
    "Each branch represents an outcome of that decision.\n",
    "Each leaf node represents the final classification or prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbb16f5-9201-4a54-bbdf-06a18ba2b133",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is Pre-Pruning in Decision Trees\n",
    "Pre-Pruning: this approach involves stopping the tree before it has completed fitting the training set. Pre-Pruning involves setting the model hyperparameters that control how large the tree can grow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adf4a4c-82b7-4451-aec4-8be1114bf9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is Post-Pruning in Decision Trees\n",
    "Pre-Pruning involves setting the model hyperparameters that control how large the tree can grow. Post-Pruning: here the tree is allowed to fit the training data perfectly, and subsequently it is truncated according to some criteria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fef649f-08eb-456a-9a34-1de6c4888343",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is the difference between Pre-Pruning and Post-Pruning\n",
    "Pre-pruning or forward pruning stops the non-significant branches from generating.\n",
    "Post-pruning or backward pruning generates the full tree and then prunes or removes the non-significant branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9926941-d591-41d3-b34f-dd73eeb23801",
   "metadata": {},
   "outputs": [],
   "source": [
    " What is a Decision Tree Regressor\n",
    " Decision Tree Regressor is a machine learning model used for predicting continuous values. It works by splitting the data into subsets based on the values of the input features, creating a tree-like structure where each internal node represents a decision based on a feature, and each leaf node represents a predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787aefe5-29a9-4d7f-99c6-35cb005be6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    " What are the advantages and disadvantages of Decision Trees\n",
    "Advantages of decision trees are: intuitive and easy to understand, flexible. Disadvantages: Overfitting, computationally expensive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f8008-cfab-496c-abad-d89c261e1f92",
   "metadata": {},
   "outputs": [],
   "source": [
    " How does a Decision Tree handle missing values\n",
    "Handling Missing Values During Training\n",
    "\n",
    "Decision trees use systematic approaches to manage missing data when building the tree:\n",
    "\n",
    "Attribute Splitting: When a feature with missing values is selected for splitting, the algorithm uses the available non-missing data to determine the best split. Instances with missing values are either ignored or assigned to branches based on available data.\n",
    "\n",
    "Weighted Impurity Calculation: The algorithm calculates impurity (e.g., Gini impurity or entropy) for both branchesâ€”one with missing values and one without. The impurity is weighted by the proportion of instances in each branch, ensuring missing values are considered in the split evaluation.\n",
    "\n",
    "Surrogate Splits: Backup rules, known as surrogate splits, are created during training. These splits use alternative features to mimic the behavior of the primary feature, ensuring robustness when missing values occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab883a25-8394-4efa-a8ef-740fedb3549a",
   "metadata": {},
   "outputs": [],
   "source": [
    " How does a Decision Tree handle categorical features\n",
    "Yes, Decision Trees handle categorical features naturally. Often these features are treated by first one-hot-encoding (OHE) in a preprocessing step. However, it is straightforward to extend the CART algorithm to make use of categorical features without such preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574fceb-2a50-4702-b461-2813a561fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    " What are some real-world applications of Decision Trees?\n",
    "The decision tree tool is used in real life in many areas, such as engineering, civil planning, law, and business. Decision trees can be divided into two types; categorical variable and continuous variable decision trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c244f37-1219-4ebe-9e50-38b80c0395ef",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94946116-3a88-4cc3-9ee9-70b142bd05e6",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the \n",
    "feature importance\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aea22e-b2a1-48a0-a348-37d21727f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the \n",
    "model accuracy\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_te_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de60b7e-730b-43e9-a92b-cb08d17b01bf",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean \n",
    "Squared Error (MSE)\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da24860d-02c9-4020-a5dc-5b5161158ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "import graphviz\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59eb441-50c1-49ab-b56a-2eac0a4bce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its \n",
    "accuracy with a fully grown tree\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, ra_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7bfc31-5640-4a8c-a31d-8ec703bb4c30",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its \n",
    "accuracy with a default tree\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split into training and testing sets (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Classifier with min_samples_spli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19259c75-b0bb-4fd3-88fb-c26aedde8a60",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its \n",
    "accuracy with unscaled data\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into training and testing sets (80%/20%)\n",
    "X_tra_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719dd18-bb93-4fa1-b8ec-80698a78bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass \n",
    "classification\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a One-vs-Rest classifier with Decision Tree as the base estimator\n",
    "ovr_clf = O_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53ee0a8-88e1-4cd0-84f5-e80b16b8b029",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train a Decision Tree Classifier and display the feature importance scores\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baad46b-404e-4f55-8050-b2ce8b46295b",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance \n",
    "with an unrestricted tree\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Split data into training and testing sets (80%/20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Decision Tree Regressor with max_depth=5\n",
    "regressor_limited = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "regressor_limited.fit(X_train, y_train)\n",
    "y_pred_limited = regressor_limited.predict(X_test)\n",
    "mse_limited = mean_squared_error(y_test, y_pred_limited)\n",
    "\n",
    "# Decision T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aa4656-39e0-4e7d-bc1c-b56b478dbcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and \n",
    "visualize its effect on accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Decision Tree to get effective alphas for pruning\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Get the effective alpha values and corresponding number of nodes for pruning\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee97ab9d-a4c7-4c4d-a82d-9f1e567daabe",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision, \n",
    "Recall, and F1-Score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate precision, recall, and F1-score (macro average for multiclass)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Precision (macro-average): {precision:.2f}\")\n",
    "print(f\"Recall (macro-average):    {recall:.2f}\")\n",
    "print(f\"F1-Score (macro-average):  {f1:.2f}\")\n",
    "\n",
    "# Optionally, print a full classification report\n",
    "print(\"\\nCl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70502397-dde8-4726-9aef-7ebe79146d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "class_names = iris.target_names\n",
    "\n",
    "# Split data into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = clf.predict_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5bcdb6-71ed-4b64-bb49-41e1b97b0798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
